{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import cPickle as pickle\n",
    "import re #Regex\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Evaluating Crawling Indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terlihat total penggunaan waktu dan memory sangat signifikan, hal tersebut karena jumlah berita positif dan negatif yang berbeda jauh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.7 s\n",
      "crawlMem     4442320.0000\n",
      "crawlTime        753.4597\n",
      "dtype: float64\n",
      "feedMem     5655162.00000\n",
      "feedTime         32.16013\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# %time DATACrawl = pickle.load( open( \"v1.4\\DATA_ANALSISIS_crawl.p\", \"rb\" ) )\n",
    "%time DATACrawl = pickle.load( open( \"v1.3\\df_mysql.p\", \"rb\" ) )\n",
    "# %time DATACrawl = pickle.load( open( \"v1.4\\DATA_CRAWL-v4b.p\", \"rb\" ) )\n",
    "Data_Analisis = DATACrawl[DATACrawl.flag.isin([-1,1])][~DATACrawl.crawlMem.isnull()]\n",
    "\n",
    "crawlIndicator = pd.DataFrame([Data_Analisis.crawlMem,Data_Analisis.crawlTime]).transpose()\n",
    "feedIndicator  = pd.DataFrame([Data_Analisis.feedMem,Data_Analisis.feedTime]).transpose()\n",
    "print crawlIndicator.sum()\n",
    "print feedIndicator.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2.Evaluating the Classifier v1.4 vs v1.6 vs v2.0\n",
    "\n",
    "\n",
    "## 2.1 Load from the 'Pickled' dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 65 ms\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time DATACrawl = pickle.load( open( \"v1.4\\DATA_CRAWL-v4b.p\", \"rb\" ) )\n",
    "Data_Analisis = DATACrawl[DATACrawl.flag.isin([-1,1])][~DATACrawl.crawlMem.isnull()]\n",
    "DATAStd  = pd.DataFrame()\n",
    "\n",
    "#Load standard Text\n",
    "DATAStd['Judul'] = Data_Analisis.judul\n",
    "DATAStd['Short'] = Data_Analisis.shortDesc\n",
    "DATAStd['Long']  = Data_Analisis.longDesc\n",
    "\n",
    "# DATAStd['CrawlMem']   = Data_Analisis.crawlMem\n",
    "# DATAStd['CrawlTime']  = Data_Analisis.crawlTime\n",
    "# DATAStd['FeedMem']    = Data_Analisis.feedMem\n",
    "# DATAStd['FeedTime']   = Data_Analisis.feedTime\n",
    "\n",
    "%time DATA_TARGET = Data_Analisis.flag\n",
    "\n",
    "DATA_TARGET = DATA_TARGET.reset_index(drop=True)\n",
    "DATAStd     = DATAStd.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Negative Instances    :  324\n",
      "Training Negative Instances :  324\n",
      "Training Positif Instances  :  146\n"
     ]
    }
   ],
   "source": [
    "#We only use 2% of total negative instance\n",
    "factor = 100.0/100\n",
    "size   = int(factor*DATA_TARGET[DATA_TARGET.values == -1].count())\n",
    "print 'Total Negative Instances    : ',DATA_TARGET[DATA_TARGET.values == -1].count()\n",
    "print 'Training Negative Instances : ',size\n",
    "print 'Training Positif Instances  : ',DATA_TARGET[DATA_TARGET.values == 1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluate\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "![smiley](img/evaluasi preproc2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import string\n",
    "import re #Regex\n",
    "def preprocK1(par):\n",
    "    \n",
    "    def onlyAZ(teks):\n",
    "        return re.sub(r'[^a-zA-Z]',' ', teks)\n",
    "    \n",
    "    class MLStripper(HTMLParser):\n",
    "        def __init__(self):\n",
    "            self.reset()\n",
    "            self.fed = []\n",
    "        def handle_data(self, d):\n",
    "            self.fed.append(d)\n",
    "        def get_data(self):\n",
    "            return ' '.join(self.fed)\n",
    "\n",
    "    def strip_tags(html):\n",
    "        s = MLStripper()\n",
    "        s.feed(html)\n",
    "        striped  = s.get_data()                         #get HTML-Tags free text\n",
    "        lowers   = striped.lower()                      #Lowercase all words\n",
    "        nospace  = lowers.strip()                       #Remove leading and trailing white space\n",
    "        return nospace\n",
    "    \n",
    "    par         = strip_tags(par)\n",
    "    onlyAZs     = onlyAZ(par)\n",
    "    return onlyAZs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<img src=\"http://cdn-media.viva.id/thumbs2/2014/11/25/281549_pekerja-perkebunan-kelapa-sawit-di-sumatra-utara_663_382.jpg\" align=\"left\" hspace=\"7\" width=\"100\" />Laba anjlok sampai 75 persen.\n",
      "laba anjlok sampai    persen \n"
     ]
    }
   ],
   "source": [
    "d = DATAStd['Short'][0]\n",
    "print d\n",
    "print preprocK1(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import string\n",
    "import re #Regex\n",
    "\n",
    "#Stopwords data source : https://www.illc.uva.nl/Research/Publications/Reports/MoL-2003-02.text.pdf\n",
    "stopword_html = open(\"id.stopwords.01.01.2016.txt\",'r').read()\n",
    "stopwords     = stopword_html.split(\"\\n\")\n",
    "\n",
    "def preprocK2(par):\n",
    "\n",
    "    postagger = pickle.load(open(\"POSTAGGER.p\", \"rb\"))\n",
    "    \n",
    "    def stopwordRemoval(text):\n",
    "        text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "        \n",
    "        return text\n",
    "    def onlyAZ(teks):\n",
    "        return re.sub(r'[^a-zA-Z]',' ', teks)\n",
    "\n",
    "    def onlyNVFromSentence(teks):\n",
    "        splitted = postagger.tag(onlyAZ(teks).split())\n",
    "        nouns = [word for word,pos in splitted \\\n",
    "            if (pos == 'NN' or pos == 'NNP' or \n",
    "                pos == 'NNS' or pos == 'VB')]\n",
    "        nounsSentence = string.join(nouns)\n",
    "        return nounsSentence\n",
    "\n",
    "    class MLStripper(HTMLParser):\n",
    "        def __init__(self):\n",
    "            self.reset()\n",
    "            self.fed = []\n",
    "        def handle_data(self, d):\n",
    "            self.fed.append(d)\n",
    "        def get_data(self):\n",
    "            return ' '.join(self.fed)\n",
    "\n",
    "    def strip_tags(html):\n",
    "        s = MLStripper()\n",
    "        s.feed(html)\n",
    "        striped  = s.get_data()                         #get HTML-Tags free text\n",
    "        lowers   = striped.lower()                      #Lowercase all words\n",
    "        nospace  = lowers.strip()                       #Remove leading and trailing white space\n",
    "        return nospace\n",
    "\n",
    "    par         = strip_tags(par)\n",
    "    par         = stopwordRemoval(par)\n",
    "    NVPar       = onlyNVFromSentence(par)\n",
    "    return  NVPar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<img src=\"http://cdn-media.viva.id/thumbs2/2014/11/25/281549_pekerja-perkebunan-kelapa-sawit-di-sumatra-utara_663_382.jpg\" align=\"left\" hspace=\"7\" width=\"100\" />Laba anjlok sampai 75 persen.\n",
      "laba anjlok\n"
     ]
    }
   ],
   "source": [
    "d = DATAStd['Short'][0]\n",
    "print d\n",
    "print preprocK2(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import string\n",
    "import re #Regex\n",
    "import operator\n",
    "def preprocK3(data):\n",
    "    \n",
    "    postagger = pickle.load(open( \"POSTAGGER.p\", \"rb\" ))\n",
    "    tagList   = pickle.load(open( \"TAGLIST.p\", \"rb\" ))\n",
    "    \n",
    "    def onlyAZ(teks):\n",
    "        return re.sub(r'[^a-zA-Z]', ' ', teks)\n",
    "    \n",
    "    def onlyNVFractionSentence(teks):\n",
    "        splitted = postagger.tag(onlyAZ(teks).split())\n",
    "        N  = len(splitted)\n",
    "        \n",
    "        word = 0\n",
    "        k = {}\n",
    "        for i in tagList:k[i] = float (0)\n",
    "        \n",
    "        for word,pos in splitted:\n",
    "            k[pos]+=1\n",
    "        return N,k\n",
    "\n",
    "    def onlyNVFractionParagraph(par):\n",
    "        splittedPar = par.split('.')\n",
    "        \n",
    "        word = 0\n",
    "        _k = {}\n",
    "        for i in tagList:_k[i] = float (0)\n",
    "        \n",
    "        for sentence in splittedPar:\n",
    "            w,k = onlyNVFractionSentence(sentence)\n",
    "            word+=w\n",
    "            _k = {x: _k.get(x, 0) + k.get(x, 0) for x in set(_k)}\n",
    "        #return noun/word,verb/word\n",
    "        return _k.values()\n",
    "    \n",
    "    class MLStripper(HTMLParser):\n",
    "        def __init__(self):\n",
    "            self.reset()\n",
    "            self.fed = []\n",
    "        def handle_data(self, d):\n",
    "            self.fed.append(d)\n",
    "        def get_data(self):\n",
    "            return ''.join(self.fed)\n",
    "\n",
    "    def strip_tags(html):\n",
    "        s = MLStripper()\n",
    "        s.feed(html)\n",
    "        striped  = s.get_data()                         #get HTML-Tags free text\n",
    "        lowers   = striped.lower()                      #Lowercase all words\n",
    "        nospace  = lowers.strip()                       #Remove leading and trailing white space\n",
    "        return nospace\n",
    "    \n",
    "    balikan = np.array([0 for i in range(0,len(tagList))])\n",
    "    for teks in data:\n",
    "        frac = onlyNVFractionParagraph(strip_tags(teks))\n",
    "        balikan = np.vstack((balikan,frac))\n",
    "    return balikan[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 65 ms\n",
      "Wall time: 11.7 s\n",
      "Wall time: 162 ms\n",
      "Wall time: 316 ms\n",
      "Wall time: 14.8 s\n",
      "Wall time: 1.06 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "dummy1 = DATAStd['Judul']+' '+DATAStd['Short']\n",
    "dummy2 = DATAStd['Judul']+' '+DATAStd['Long']+' '+DATAStd['Short']\n",
    "\n",
    "vect = TfidfVectorizer(decode_error='ignore')\n",
    "\n",
    "%time d1k1 = vect.fit_transform(dummy1.map(lambda x: preprocK1(x)))\n",
    "%time d1k2 = vect.fit_transform(dummy1.map(lambda x: preprocK2(x)))\n",
    "%time d1k3 = preprocK3(dummy1)\n",
    "%time d2k1 = vect.fit_transform(dummy2.map(lambda x: preprocK1(x)))\n",
    "%time d2k2 = vect.fit_transform(dummy2.map(lambda x: preprocK2(x)))\n",
    "%time d2k3 = preprocK3(dummy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<470x9725 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 46062 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d1k1\n",
    "# d1k2\n",
    "\n",
    "# d2k1\n",
    "# d2k2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4442320.0\n",
      "753.4597\n",
      "5655162\n",
      "32.1601297744\n"
     ]
    }
   ],
   "source": [
    "print DATAStd.CrawlMem.sum()\n",
    "print DATAStd.CrawlTime.sum()\n",
    "print DATAStd.FeedMem.sum()\n",
    "print DATAStd.FeedTime.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn import cross_validation\n",
    "# from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "# from sklearn.metrics import precision_score, recall_score, classification_report, roc_auc_score,roc_curve,auc\n",
    "# from scipy import interp\n",
    "# import datetime\n",
    "\n",
    "# def skorStratCV(clf,X,y,nfolds=10):\n",
    "#     metrics = []\n",
    "    \n",
    "#     trainTime,testTime = [],[]\n",
    "#     precis,recals,f1s,accs,rocs = [],[],[],[],[]\n",
    "    \n",
    "#     mean_tpr = 0.0\n",
    "#     mean_fpr = np.linspace(0, 1, 100)\n",
    "#     all_tpr = []\n",
    "    \n",
    "#     skf = cross_validation.StratifiedKFold(y, n_folds=nfolds)\n",
    "#     for train_index, test_index in skf:\n",
    "#         X_train,y_train = X[train_index],y[train_index]\n",
    "#         X_test,y_test   = X[test_index],y[test_index]\n",
    "        \n",
    "#         #training time\n",
    "#         start = dt.now()\n",
    "#         clf.fit(X_train, y_train)\n",
    "#         end = dt.now()\n",
    "#         trainTime.append((end-start).microseconds)\n",
    "        \n",
    "#         #testing time\n",
    "#         start = dt.now()\n",
    "#         yhat = clf.predict(X_test)\n",
    "#         end = dt.now()\n",
    "#         testTime.append((end-start).microseconds)\n",
    "        \n",
    "#         #score\n",
    "#         precis.append(precision_score(y_test, yhat, average='binary'))\n",
    "#         recals.append(recall_score(y_test, yhat, average='binary'))\n",
    "#         f1s.append(f1_score(y_test, yhat, average='binary'))\n",
    "#         accs.append(accuracy_score(y_test, yhat))\n",
    "        \n",
    "#         probas_              = clf.fit(X_train, y_train).predict_proba(X_test)\n",
    "#         fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1])\n",
    "#         mean_tpr            += interp(mean_fpr, fpr, tpr)\n",
    "#         mean_tpr[0]          = 0.0\n",
    "        \n",
    "#     def ave(lis):\n",
    "#         return sum(lis)/len(lis)\n",
    "    \n",
    "#     metrics.append(ave(trainTime))\n",
    "#     metrics.append(ave(testTime))\n",
    "\n",
    "# #     metrics.append(ave(testTime))\n",
    "#     metrics.append(ave(precis))\n",
    "#     metrics.append(ave(recals))\n",
    "#     metrics.append(ave(f1s))\n",
    "#     metrics.append(ave(accs))\n",
    "    \n",
    "#     mean_tpr /= nfolds\n",
    "#     mean_tpr[-1] = 1.0\n",
    "#     mean_auc = auc(mean_fpr, mean_tpr)\n",
    "#     metrics.append(mean_auc)\n",
    "    \n",
    "#     return metrics,mean_fpr,mean_tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# mnb = MultinomialNB(alpha=0.1)\n",
    "# svc = SVC(kernel='linear',cache_size=800,probability=True)\n",
    "\n",
    "# kombinasi  = [{'name' : 'MNB', 'pipe' : mnb},\n",
    "#               {'name' : 'SVM', 'pipe' : svc},\n",
    "#              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# kombinasi[0]['pipe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X1 = []\n",
    "# # X0.append({'name':'Judul','Prep':'Kombinasi1','Data':DATAStd['Judul']})\n",
    "# # X0.append({'name':'Short','Prep':'Kombinasi1','Data':DATAStd['Short']})\n",
    "# # X0.append({'name':'Long' ,'Prep':'Kombinasi1','Data':DATAStd['Long']})\n",
    "# X1.append({'name':'X1' ,'Prep':'K1','Data':d1k1})\n",
    "# X1.append({'name':'X2' ,'Prep':'K1','Data':d2k1})\n",
    "\n",
    "# X2 = []\n",
    "# # X0.append({'name':'Judul','Prep':'Kombinasi1','Data':DATAStd['Judul']})\n",
    "# # X0.append({'name':'Short','Prep':'Kombinasi1','Data':DATAStd['Short']})\n",
    "# # X0.append({'name':'Long' ,'Prep':'Kombinasi1','Data':DATAStd['Long']})\n",
    "# X2.append({'name':'X1' ,'Prep':'K2','Data':d1k2})\n",
    "# X2.append({'name':'X2' ,'Prep':'K2','Data':d2k2})\n",
    "\n",
    "# X3 = []\n",
    "# # X0.append({'name':'Judul','Prep':'Kombinasi1','Data':DATAStd['Judul']})\n",
    "# # X0.append({'name':'Short','Prep':'Kombinasi1','Data':DATAStd['Short']})\n",
    "# # X0.append({'name':'Long' ,'Prep':'Kombinasi1','Data':DATAStd['Long']})\n",
    "# X3.append({'name':'X1' ,'Prep':'K3','Data':d1k3})\n",
    "# X3.append({'name':'X2' ,'Prep':'K3','Data':d2k3})\n",
    "\n",
    "# y_  = DATA_TARGET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kombinasi 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# metric = []\n",
    "# for xi in X1:\n",
    "#     X_ = xi['Data']\n",
    "#     for mod in kombinasi:\n",
    "# #         xP = X_[y_[y_.values == 1].index]\n",
    "# #         xN = X_[y_[y_.values == -1].index][:size]\n",
    "# #         X  = np.append(xP,xN)\n",
    "        \n",
    "# #         yP = y_[y_.values == 1]\n",
    "# #         yN = y_[y_.values == -1][:size]\n",
    "# #         y  = np.append(yP,yN)\n",
    "        \n",
    "#         X = X_\n",
    "#         y = y_\n",
    "        \n",
    "#         metrics,mean_fpr,mean_tpr = skorStratCV(mod['pipe'],X,y)\n",
    "#         name = xi['name']+' '+xi['Prep']+' '+mod['name']\n",
    "#         plt.plot(mean_fpr, mean_tpr,label='Mean AUC %s (area = %0.2f)' % (name,metrics[5]), lw=2)\n",
    "        \n",
    "#         metric.append({'name':name,'metrics':metrics})\n",
    "\n",
    "# plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n",
    "# plt.xlim([-0.05, 1.05])\n",
    "# plt.ylim([-0.05, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic of Kombinasi1')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kombinasi 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# metric = []\n",
    "# for xi in X2:\n",
    "#     X_ = xi['Data']\n",
    "#     for mod in kombinasi:\n",
    "# #         xP = X_[y_[y_.values == 1].index]\n",
    "# #         xN = X_[y_[y_.values == -1].index][:size]\n",
    "# #         X  = np.append(xP,xN)\n",
    "        \n",
    "# #         yP = y_[y_.values == 1]\n",
    "# #         yN = y_[y_.values == -1][:size]\n",
    "# #         y  = np.append(yP,yN)\n",
    "        \n",
    "#         X = X_\n",
    "#         y = y_\n",
    "        \n",
    "#         metrics,mean_fpr,mean_tpr = skorStratCV(mod['pipe'],X,y)\n",
    "#         name = xi['name']+' '+xi['Prep']+' '+mod['name']\n",
    "#         plt.plot(mean_fpr, mean_tpr,label='Mean AUC %s (area = %0.2f)' % (name,metrics[5]), lw=2)\n",
    "        \n",
    "#         metric.append({'name':name,'metrics':metrics})\n",
    "\n",
    "# plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n",
    "# plt.xlim([-0.05, 1.05])\n",
    "# plt.ylim([-0.05, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic of Kombinasi1')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# metDum = metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kombinasi 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# metric = []\n",
    "# for xi in X3:\n",
    "#     X_ = xi['Data']\n",
    "#     for mod in kombinasi:\n",
    "# #         xP = X_[y_[y_.values == 1].index]\n",
    "# #         xN = X_[y_[y_.values == -1].index][:size]\n",
    "# #         X  = np.append(xP,xN)\n",
    "        \n",
    "# #         yP = y_[y_.values == 1]\n",
    "# #         yN = y_[y_.values == -1][:size]\n",
    "# #         y  = np.append(yP,yN)\n",
    "        \n",
    "#         X = X_\n",
    "#         y = y_\n",
    "        \n",
    "#         metrics,mean_fpr,mean_tpr = skorStratCV(mod['pipe'],X,y)\n",
    "#         name = xi['name']+' '+xi['Prep']+' '+mod['name']\n",
    "#         plt.plot(mean_fpr, mean_tpr,label='Mean AUC %s (area = %0.2f)' % (name,metrics[5]), lw=2)\n",
    "        \n",
    "#         metric.append({'name':name,'metrics':metrics})\n",
    "\n",
    "# plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n",
    "# plt.xlim([-0.05, 1.05])\n",
    "# plt.ylim([-0.05, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic of Kombinasi1')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from ipy_table import *\n",
    "# metrics_table = []\n",
    "# tables = []\n",
    "# columns = (['id', 'nama','time','p_1',\n",
    "#             'r_1','F1 score','accuracy','AUC'])\n",
    "# tables.append(columns)\n",
    "\n",
    "# i = 0\n",
    "\n",
    "# for me in metric:\n",
    "#     i += 1\n",
    "    \n",
    "#     metr = []\n",
    "#     metr.append(i)\n",
    "#     metr.append(me['name'])\n",
    "#     for m in me['metrics']:\n",
    "#         metr.append(m)\n",
    "\n",
    "#     tables.append(metr)\n",
    "#     metrics_table.append(metr)\n",
    "#     #print metric\n",
    "    \n",
    "# make_table(tables)\n",
    "\n",
    "# # styling\n",
    "# apply_theme('basic_both')\n",
    "# set_column_style(11, align='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evals = pd.DataFrame(metrics_table,columns=columns)\n",
    "# evals['Vars'] = [i.split()[0] for i in evals['nama']]\n",
    "# evals['Prep'] = [i.split()[1] for i in evals['nama']]\n",
    "# evals['Algs'] = [i.split()[2] for i in evals['nama']]\n",
    "# # evals['Time'] = evals['time']\n",
    "# evals.drop('nama',axis=1,inplace=True)\n",
    "# evals.drop('id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evals.to_csv('Hasil3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # splitter1 = ['Judul','Short','Long']\n",
    "# splitter1 = ['Feed1','Feed2']\n",
    "# # splitter2 = ['Kombinasi1','Kombinasi2','Kombinasi3','Kombinasi4','Kombinasi5']\n",
    "# splitter2 = ['Kombinasi1','Kombinasi2','Kombinasi3']\n",
    "# eval_teks = evals[evals.Prep.isin(splitter2)]\n",
    "# eval_teks = pd.DataFrame([eval_teks[eval_teks.Vars == i].mean() for i in splitter1]).transpose()\n",
    "# eval_teks.set_axis(1,splitter1)\n",
    "# eval_teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # splitter = ['Kombinasi1','Kombinasi2','Kombinasi3','Kombinasi4','Kombinasi5']\n",
    "# splitter = ['Kombinasi1','Kombinasi2','Kombinasi3']\n",
    "# eval_prep = pd.DataFrame([evals[evals.Prep == i].mean() for i in splitter]).transpose()\n",
    "# eval_prep.set_axis(1,splitter)\n",
    "# eval_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# splitter1 = ['MNB','SVM']\n",
    "# eval_teks = evals[evals.Algs.isin(splitter1)]\n",
    "# eval_teks = pd.DataFrame([eval_teks[eval_teks.Algs == i].mean() for i in splitter1]).transpose()\n",
    "# eval_teks.set_axis(1,splitter1)\n",
    "# eval_teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#knn\n",
    "# knn_params = {'n_neighbors': range(1,21), 'weights': ['uniform', 'distance'], 'algorithm': ['ball_tree', 'kd_tree'],\n",
    "#               'leaf_size': [15, 30, 50, 100], 'p': [1,2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc\n",
    "# lsvm_params = {'loss':['hinge', 'squared_hinge'],'class_weight':['balanced']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tre\n",
    "# tree_param = {\"criterion\": [\"gini\", \"entropy\"],\"class_weight\" : ['balanced']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def skorStratCV2(clf,X,y,feedTime,feedMem,crawlTime,crawlMem,nfolds=10):\n",
    "#     metrics = []\n",
    "    \n",
    "#     trainTime,testTime = [],[]\n",
    "#     precis,recals,f1s,accs,rocs = [],[],[],[],[]\n",
    "    \n",
    "#     mean_tpr = 0.0\n",
    "#     mean_fpr = np.linspace(0, 1, 100)\n",
    "#     all_tpr = []\n",
    "#     X = preprocK3(X)\n",
    "#     skf = cross_validation.StratifiedKFold(y, n_folds=nfolds)\n",
    "#     for train_index, test_index in skf:\n",
    "#         X_train,y_train = X[train_index],y[train_index]\n",
    "#         X_test,y_test   = X[test_index],y[test_index]\n",
    "        \n",
    "#         #training time\n",
    "#         start = dt.now()\n",
    "#         clf.fit(X_train, y_train)\n",
    "#         end = dt.now()\n",
    "#         trainTime.append((end-start).microseconds)\n",
    "        \n",
    "#         #testing time\n",
    "#         start = dt.now()\n",
    "#         yhat = clf.predict(X_test)\n",
    "#         end = dt.now()\n",
    "#         testTime.append((end-start).microseconds)\n",
    "        \n",
    "#         #score\n",
    "#         precis.append(precision_score(y_test, yhat, average='binary'))\n",
    "#         recals.append(recall_score(y_test, yhat, average='binary'))\n",
    "#         f1s.append(f1_score(y_test, yhat, average='binary'))\n",
    "#         accs.append(accuracy_score(y_test, yhat))\n",
    "        \n",
    "#         probas_              = clf.fit(X_train, y_train).predict_proba(X_test)\n",
    "#         fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1])\n",
    "#         mean_tpr            += interp(mean_fpr, fpr, tpr)\n",
    "#         mean_tpr[0]          = 0.0\n",
    "        \n",
    "#     def ave(lis):\n",
    "#         return sum(lis)/len(lis)\n",
    "    \n",
    "#     testTime = ave(trainTime)+ave(testTime)\n",
    "    \n",
    "#     metrics.append(testTime)\n",
    "#     metrics.append(feedTime)\n",
    "#     metrics.append(feedMem)\n",
    "#     metrics.append(crawlTime)\n",
    "#     metrics.append(crawlMem)\n",
    "\n",
    "# #     metrics.append(ave(testTime))\n",
    "#     metrics.append(ave(precis))\n",
    "#     metrics.append(ave(recals))\n",
    "#     metrics.append(ave(f1s))\n",
    "#     metrics.append(ave(accs))\n",
    "    \n",
    "#     mean_tpr /= nfolds\n",
    "#     mean_tpr[-1] = 1.0\n",
    "#     mean_auc = auc(mean_fpr, mean_tpr)\n",
    "#     metrics.append(mean_auc)\n",
    "    \n",
    "#     return metrics,mean_fpr,mean_tpr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
