{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import cPickle as pickle\n",
    "import re #Regex\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 181 ms\n",
      "Wall time: 44 ms\n",
      "Wall time: 265 ms\n",
      "Wall time: 55 ms\n",
      "Wall time: 36 ms\n",
      "Wall time: 188 ms\n",
      "Wall time: 164 ms\n",
      "Wall time: 165 ms\n",
      "Wall time: 159 ms\n",
      "Wall time: 32 ms\n"
     ]
    }
   ],
   "source": [
    "DATAStd  = pd.DataFrame()\n",
    "DATAPost = pd.DataFrame()\n",
    "\n",
    "#Load standard striped Text\n",
    "%time DATAStd['Judul'] = pickle.load( open( \"v1.4\\DATA_JUDULStriped-v4a.p\", \"rb\" ) )\n",
    "%time DATAStd['Short'] = pickle.load( open( \"v1.4\\DATA_SHORTStriped-v4a.p\", \"rb\" ) )\n",
    "%time DATAStd['Long']  = pickle.load( open( \"v1.4\\DATA_LONGStriped-v4a.p\", \"rb\" ) )\n",
    "\n",
    "#Load from PosTagged Noun and verb only Text\n",
    "%time DATAPost['Judul'] = pickle.load( open( \"v1.6\\DATA_JUDULpostag-v5a.p\", \"rb\" ) )\n",
    "%time DATAPost['Short'] = pickle.load( open( \"v1.6\\DATA_SHORTpostag-v5a.p\", \"rb\" ) )\n",
    "%time DATAPost['Long']  = pickle.load( open( \"v1.6\\DATA_LONGpostag-v5a.p\", \"rb\" ) )\n",
    "\n",
    "#Load number of total tag in Text\n",
    "%time AllFracJudul = pickle.load( open( \"v2.0\\DATA_JUDUL-v6a.p\", \"rb\" ) )\n",
    "%time AllFracShort = pickle.load( open( \"v2.0\\DATA_SHORT-v6a.p\", \"rb\" ) )\n",
    "%time AllFracLong  = pickle.load( open( \"v2.0\\DATA_LONG-v6a.p\", \"rb\" ) )\n",
    "\n",
    "#Load number of total noun-verb only in Text\n",
    "# %time NVFracJudul = pickle.load( open( \"v2.0\\DATA_JUDUL-v2.p\", \"rb\" ) )\n",
    "# %time NVFracShort = pickle.load( open( \"v2.0\\DATA_SHORT-v2.p\", \"rb\" ) )\n",
    "# %time NVFracLong  = pickle.load( open( \"v2.0\\DATA_LONG-v2.p\", \"rb\" ) )\n",
    "\n",
    "%time DATA_TARGET = pickle.load( open( \"v1.4\\DATA_TARGET-v4a.p\", \"rb\" ) )\n",
    "\n",
    "DATA_TARGET = DATA_TARGET.reset_index(drop=True)\n",
    "DATAStd  = DATAStd.reset_index(drop=True)\n",
    "DATAPost = DATAPost.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Negative Instances    :  945\n",
      "Training Negative Instances :  945\n",
      "Training Positif Instances  :  399\n"
     ]
    }
   ],
   "source": [
    "#We only use 2% of total negative instance\n",
    "factor = 100/100\n",
    "size   = int(factor*DATA_TARGET[DATA_TARGET.values == -1].count())\n",
    "print 'Total Negative Instances    : ',DATA_TARGET[DATA_TARGET.values == -1].count()\n",
    "print 'Training Negative Instances : ',size\n",
    "print 'Training Positif Instances  : ',DATA_TARGET[DATA_TARGET.values == 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#Stopwords data source : https://www.illc.uva.nl/Research/Publications/Reports/MoL-2003-02.text.pdf\n",
    "stopword_html = open(\"id.stopwords.01.01.2016.txt\",'r').read()\n",
    "stopwords     = stopword_html.split(\"\\n\")\n",
    "\n",
    "vect1 = TfidfVectorizer(decode_error='ignore')\n",
    "vect2 = TfidfVectorizer(stop_words=stopwords,decode_error='ignore')\n",
    "\n",
    "vectStds = vect1.fit_transform(DATAStd['Long'])\n",
    "vectStop = vect2.fit_transform(DATAStd['Long'])\n",
    "vectPost = vect1.fit_transform(DATAPost['Long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "mnb = MultinomialNB(alpha=0.1)\n",
    "#knn = KNeighborsClassifier()\n",
    "svc = SVC(kernel='linear',cache_size=800,probability=True)\n",
    "#tre = DecisionTreeClassifier(class_weight='balanced')\n",
    "\n",
    "vectCnts = TfidfVectorizer(decode_error='ignore')\n",
    "vectStds = TfidfVectorizer(decode_error='ignore')\n",
    "#vectStop = TfidfVectorizer(stop_words=stopwords,decode_error='ignore')\n",
    "\n",
    "kombinasi13 = [{'name' : 'MNB', 'pipe' : Pipeline([('vect', vectCnts),('clf', mnb)])},\n",
    "               #{'name' : 'KNN', 'pipe' : Pipeline([('vect', vect),('clf', knn)])},\n",
    "               {'name' : 'SVM', 'pipe' : Pipeline([('vect', vectCnts),('clf', svc)])},\n",
    "               #{'name' : 'TREE', 'pipe' : Pipeline([('vect', vect),('clf', tre)])}\n",
    "              ]\n",
    "\n",
    "kombinasi24= [{'name' : 'MNB', 'pipe' : Pipeline([('vect', vectStds),('clf', mnb)])},\n",
    "             #{'name' : 'KNN', 'pipe' : Pipeline([('vect', vect),('clf', knn)])},\n",
    "             {'name' : 'SVM', 'pipe' : Pipeline([('vect', vectStds),('clf', svc)])},\n",
    "             #{'name' : 'TREE', 'pipe' : Pipeline([('vect', vect),('clf', tre)])}\n",
    "             ]\n",
    "\n",
    "kombinasi5 = [{'name' : 'MNB', 'pipe' : mnb},\n",
    "               {'name' : 'SVM', 'pipe' : svc},\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memakan makanan\n",
      "hasil kunjungan menteri rini ke eropa, gandeng rolls royce\n",
      "hasil kunjungan menteri rini eropa, gandeng rolls royce\n"
     ]
    }
   ],
   "source": [
    "k = 'aku akan memakan semua makanan itu'\n",
    "\n",
    "def stopwordRemoval(text):\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text\n",
    "\n",
    "print stopwordRemoval(k)\n",
    "JudulStop = DATAStd['Judul'].map(lambda x: stopwordRemoval(x))\n",
    "ShortStop = DATAStd['Short'].map(lambda x: stopwordRemoval(x))\n",
    "LongStop  = DATAStd['Long'].map(lambda x: stopwordRemoval(x))\n",
    "Feed1Stop  = JudulStop+' '+ShortStop\n",
    "Feed2Stop  = JudulStop+' '+LongStop+' '+ShortStop\n",
    "\n",
    "JudulPostStop = DATAPost['Judul'].map(lambda x: stopwordRemoval(x))\n",
    "ShortPostStop = DATAPost['Short'].map(lambda x: stopwordRemoval(x))\n",
    "LongPostStop  = DATAPost['Long'].map(lambda x: stopwordRemoval(x))\n",
    "Feed1PostStop  = JudulPostStop+' '+ShortPostStop\n",
    "Feed2PostStop  = JudulPostStop+' '+LongPostStop+' '+ShortPostStop\n",
    "\n",
    "print DATAStd['Judul'][198]\n",
    "print JudulStop[198]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X0 = []\n",
    "# X0.append({'name':'Judul','Prep':'Kombinasi1','Data':DATAStd['Judul']})\n",
    "# X0.append({'name':'Short','Prep':'Kombinasi1','Data':DATAStd['Short']})\n",
    "# X0.append({'name':'Long' ,'Prep':'Kombinasi1','Data':DATAStd['Long']})\n",
    "X0.append({'name':'X1' ,'Prep':'Kombinasi1','Data':DATAStd['Judul']+' '+DATAStd['Short']})\n",
    "X0.append({'name':'X2' ,'Prep':'Kombinasi1','Data':DATAStd['Judul']+' '+DATAStd['Long']+' '+DATAStd['Short']})\n",
    "\n",
    "\n",
    "# X1 = []\n",
    "# X1.append({'name':'Judul','Prep':'Kombinasi2','Data':JudulStop})\n",
    "# X1.append({'name':'Short','Prep':'Kombinasi2','Data':ShortStop})\n",
    "# X1.append({'name':'Long' ,'Prep':'Kombinasi2','Data':LongStop})\n",
    "# X1.append({'name':'Feed' ,'Prep':'Kombinasi2','Data':FeedStop})\n",
    "\n",
    "# X2 = []\n",
    "# X2.append({'name':'Judul','Prep':'Kombinasi3','Data':DATAPost['Judul']})\n",
    "# X2.append({'name':'Short','Prep':'Kombinasi3','Data':DATAPost['Short']})\n",
    "# X2.append({'name':'Long' ,'Prep':'Kombinasi3','Data':DATAPost['Long']})\n",
    "# X2.append({'name':'Feed' ,'Prep':'Kombinasi3','Data':DATAPost['Judul']+' '+DATAPost['Short']})\n",
    "\n",
    "X3 = []\n",
    "# X3.append({'name':'Judul','Prep':'Kombinasi4','Data':JudulPostStop})\n",
    "# X3.append({'name':'Short','Prep':'Kombinasi4','Data':ShortPostStop})\n",
    "# X3.append({'name':'Long' ,'Prep':'Kombinasi4','Data':LongPostStop})\n",
    "X3.append({'name':'X1' ,'Prep':'Kombinasi2','Data':Feed1PostStop})\n",
    "X3.append({'name':'X2' ,'Prep':'Kombinasi2','Data':Feed2PostStop})\n",
    "\n",
    "X4 = []\n",
    "# X4.append({'name':'Judul','Prep':'Kombinasi5','Data':AllFracJudul})\n",
    "# X4.append({'name':'Short','Prep':'Kombinasi5','Data':AllFracShort})\n",
    "# X4.append({'name':'Long' ,'Prep':'Kombinasi5','Data':AllFracLong})\n",
    "X4.append({'name':'X1' ,'Prep':'Kombinasi3','Data':AllFracJudul+AllFracShort})\n",
    "X4.append({'name':'X2' ,'Prep':'Kombinasi3','Data':AllFracJudul+AllFracLong+AllFracShort})\n",
    "\n",
    "# X2.append({'name':'Judul','Prep':'NVTag','Data':NVFracJudul})\n",
    "# X2.append({'name':'Short','Prep':'NVTag','Data':NVFracShort})\n",
    "# X2.append({'name':'Long' ,'Prep':'NVTag','Data':NVFracLong})\n",
    "\n",
    "y_  = DATA_TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import grid_search\n",
    "mnb_params = {'clf__alpha':[i*0.1 for i in range(10)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k13  = grid_search.GridSearchCV(kombinasi13[0]['pipe'],  mnb_params, cv=10,n_jobs=-1, scoring='roc_auc')\n",
    "k24 = grid_search.GridSearchCV(kombinasi24[0]['pipe'], mnb_params, cv=10,n_jobs=-1, scoring='roc_auc')\n",
    "k5  = grid_search.GridSearchCV(kombinasi5[0]['pipe'],  mnb_params, cv=10,n_jobs=-1, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer(analyzer=u'word', binary=False, decode_error='ignore',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True...rue,\n",
       "        vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kombinasi1[0]['pipe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "for xi in X0:\n",
    "    X_ = xi['Data']\n",
    "    xP = X_[y_[y_.values == 1].index]\n",
    "    xN = X_[y_[y_.values == -1].index][:size]\n",
    "    X  = np.append(xP,xN)\n",
    "\n",
    "    yP = y_[y_.values == 1]\n",
    "    yN = y_[y_.values == -1][:size]\n",
    "    y  = np.append(yP,yN)\n",
    "\n",
    "    k13.fit(X,y)\n",
    "    best_parameters = k13.best_estimator_.get_params()\n",
    "    print best_parameters['clf__alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "for xi in X3:\n",
    "    X_ = xi['Data']\n",
    "    xP = X_[y_[y_.values == 1].index]\n",
    "    xN = X_[y_[y_.values == -1].index][:size]\n",
    "    X  = np.append(xP,xN)\n",
    "\n",
    "    yP = y_[y_.values == 1]\n",
    "    yN = y_[y_.values == -1][:size]\n",
    "    y  = np.append(yP,yN)\n",
    "\n",
    "    k24.fit(X,y)\n",
    "    best_parameters = k24.best_estimator_.get_params()\n",
    "    print best_parameters['clf__alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "for xi in X3:\n",
    "    X_ = xi['Data']\n",
    "    xP = X_[y_[y_.values == 1].index]\n",
    "    xN = X_[y_[y_.values == -1].index][:size]\n",
    "    X  = np.append(xP,xN)\n",
    "\n",
    "    yP = y_[y_.values == 1]\n",
    "    yN = y_[y_.values == -1][:size]\n",
    "    y  = np.append(yP,yN)\n",
    "\n",
    "    k13.fit(X,y)\n",
    "    best_parameters = k13.best_estimator_.get_params()\n",
    "    print best_parameters['clf__alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y2_  = pickle.load( open( \"v2.0\\DATA_TARGET-v5b.p\", \"rb\" ) )\n",
    "for xi in X5:\n",
    "    X_ = xi['Data']\n",
    "    xP = X_[y2_[y2_.values == 1].index]\n",
    "    xN = X_[y2_[y2_.values == -1].index][:size]\n",
    "    X  = np.vstack((xP,xN))\n",
    "\n",
    "    yP = y2_[y2_.values == 1]\n",
    "    yN = y2_[y2_.values == -1][:size]\n",
    "    y  = np.append(yP,yN)\n",
    "\n",
    "    k5.fit(X,y)\n",
    "    best_parameters = k5.best_estimator_.get_params()\n",
    "    print best_parameters['clf__alpha'],k4.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
